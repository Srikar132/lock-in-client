# ğŸ¤ Lumo Voice Assistant - Implementation Guide

## Overview
A production-ready voice assistant integrated into your LOCK-IN app, featuring:
- âœ… **Full-duplex streaming** (200-300ms latency)
- âœ… **Real-time STT, LLM, and TTS** via OpenAI Realtime API
- âœ… **Barge-in support** - interrupt anytime
- âœ… **Voice Activity Detection** (VAD)
- âœ… **Real-time audio visualization**
- âœ… **Reactive state management** with Riverpod

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Microphone  â”‚ â†’ PCM audio (24kHz)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AudioStreamService       â”‚ â†’ Continuous streaming
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RealtimeService (WebSocket)       â”‚ â†’ OpenAI Realtime API
â”‚ â€¢ Streaming STT                   â”‚
â”‚ â€¢ Streaming LLM tokens            â”‚
â”‚ â€¢ Streaming TTS audio             â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AudioPlayerService       â”‚ â†’ Play while generating
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Nothing waits for completion. Everything streams.**

---

## ğŸ“ Project Structure

```
lib/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ voice_api_config.dart          # API configuration
â”œâ”€â”€ models/
â”‚   â””â”€â”€ voice_state.dart                # State models
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ audio_stream_service.dart       # Microphone capture
â”‚   â”œâ”€â”€ audio_player_service.dart       # Audio playback
â”‚   â””â”€â”€ realtime_service.dart           # WebSocket connection
â”œâ”€â”€ presentation/
â”‚   â”œâ”€â”€ screens/
â”‚   â”‚   â””â”€â”€ lumo_voice_bot_screen.dart  # Main UI
â”‚   â””â”€â”€ providers/
â”‚       â””â”€â”€ voice_session_provider.dart # State management
```

---

## ğŸš€ Setup Instructions

### 1. Install Dependencies

```bash
flutter pub get
```

### 2. Configure OpenAI API Key

**Option A: Environment Variable (Recommended)**

Run with:
```bash
flutter run --dart-define=OPENAI_API_KEY=sk-your-key-here
```

**Option B: Direct Configuration (Development Only)**

Open `lib/config/voice_api_config.dart` and replace:
```dart
static const String apiKey = 'YOUR_OPENAI_API_KEY';
```

âš ï¸ **NEVER commit API keys to version control!**

### 3. Platform-Specific Setup

#### Android
Permissions already added to `AndroidManifest.xml`:
- âœ… RECORD_AUDIO
- âœ… MODIFY_AUDIO_SETTINGS
- âœ… INTERNET

#### iOS
Permissions already added to `Info.plist`:
- âœ… NSMicrophoneUsageDescription
- âœ… NSSpeechRecognitionUsageDescription
- âœ… Background audio mode

### 4. Run the App

```bash
# Debug mode
flutter run

# Release mode (better performance)
flutter run --release
```

---

## ğŸ¯ How It Works

### Audio Flow Pipeline

1. **Microphone** captures PCM audio at 24kHz
2. **Audio chunks** sent to OpenAI Realtime API via WebSocket
3. **STT** transcribes speech in real-time (partial transcripts)
4. **LLM** generates response tokens as they arrive
5. **TTS** synthesizes audio chunks immediately
6. **Audio plays** while next chunks generate

### State Machine

```
IDLE â†’ LISTENING â†’ THINKING â†’ SPEAKING â†’ IDLE
         â†‘            â†“
         â””â”€ INTERRUPT â”€â”˜
```

---

## ğŸ“± Usage in Your App

### Navigate to Voice Screen

```dart
Navigator.push(
  context,
  MaterialPageRoute(
    builder: (context) => const LumoVoiceBotScreen(),
  ),
);
```

### Or use named routes:

```dart
// In main.dart
MaterialApp(
  routes: {
    '/voice': (context) => const LumoVoiceBotScreen(),
  },
);

// Navigate
Navigator.pushNamed(context, '/voice');
```

---

## ğŸ› Troubleshooting

### No audio captured

**Check permissions:**
```bash
# Android
adb shell pm grant com.example.lock_in android.permission.RECORD_AUDIO

# iOS
Reset permissions in Settings â†’ Privacy â†’ Microphone
```

### WebSocket connection fails

1. âœ… Verify API key is correct
2. âœ… Check internet connection
3. âœ… Ensure OpenAI API access (requires paid account)
4. âœ… Check firewall/proxy settings

### High latency

- Use **release build**: `flutter run --release`
- Close background apps
- Use wired internet if possible

### Audio playback issues

- Verify device audio output
- Check volume settings
- Use physical device (not simulator for best results)

---

## ğŸ”’ Security Best Practices

1. âœ… **Never commit API keys**
   ```bash
   # Add to .gitignore
   *.env
   **/voice_api_config.dart
   ```

2. âœ… **Use environment variables**
   ```bash
   flutter run --dart-define=OPENAI_API_KEY=your-key
   ```

3. âœ… **Implement rate limiting** (backend proxy recommended)

4. âœ… **Add user authentication** before API access

5. âœ… **Use HTTPS/WSS only**

---

## ğŸ“„ Dependencies

```yaml
dependencies:
  flutter_riverpod: ^3.0.3      # State management
  record: ^5.2.1                 # Audio recording
  audioplayers: ^6.0.0          # Audio playback
  web_socket_channel: ^2.4.0    # WebSocket
  http: ^1.2.0                   # HTTP requests
  path_provider: ^2.1.2          # File storage
  uuid: ^4.3.3                   # Unique IDs
```

---

## âš¡ Quick Start Commands

```bash
# Install dependencies
flutter pub get

# Run with API key
flutter run --dart-define=OPENAI_API_KEY=sk-your-key-here

# Build release
flutter build apk --release

# Clean build
flutter clean && flutter pub get
```

---

**That's it! You now have a production-ready voice assistant! ğŸ‰**

For questions or improvements, check the code comments or OpenAI documentation.
